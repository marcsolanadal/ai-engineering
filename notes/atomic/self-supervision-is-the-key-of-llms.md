# Self-supervision is the key of LLMs
---

> The first model that worked with *supervised* learning was AlexNet -- A convolutional neural network that was able to classify image into 1000 categories.

Before LLMs the method to train ML algorithms was to use *supervised* learning. It contisted in curating a dataset of images and labels for those images in order to teach the AI. Having humans manually labeling images doesn't scale as it's prohibitibly expensive.

The main advantage of LLMs over other ML algorithms is the usage of *self-supervised* learning. In this paradigm the data is automatically labeled. This is specially true for text since text is labeled implicitly.
