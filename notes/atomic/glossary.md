# Glossary of concepts

- *token*: Basic unit of a LLM
- *tokenization*: The process of breaking the original text into tokens. An average token is 3/4 the size of a word.
- *masked language model*: Trained to predict missing tokens before and after the missing tokens. I.e. "My favorite ___ is blue" should predict the missing word is color. They're commonly used for non-generative tasks such as sentiment analysis or code debugging.
- *autoregressive language model*: Trained to predict the next work in a sequence.
- *supervised learning*: ML algorithms trained on labeled data.
- *self-supervised learning*: ML algorithms infer labels for the input data.
- *unsupervised learning*: The algorithm don't need labels at all.
- *foundation models*: also called multi-modal models
- *prompt engineering*: The process of creating prompts for our models.
- *RAG*: Acronym of Reatrival Augmented Generation.
- *finetunning* is the process of further training the model on a high quality dataset to perform an specific task.
- *agents*: AI's that can plan and use tools.
- *human-in-the-loop*: When an AI agent involves humans in the decision making process.
- *inference*: The process of getting an output given an input.
- *quantization*: The process of reducing the precision of model weights.
